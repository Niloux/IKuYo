import datetime
import logging
import re
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup, Tag

from models import Anime, CrawlLog, Resource, SubtitleGroup

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)


# é…ç½®å¸¸é‡
class Config:
    BASE_URL = "https://mikanani.me"
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
    }
    TIMEOUT = 10
    MAX_RETRIES = 3


# CSSé€‰æ‹©å™¨å’Œæ­£åˆ™è¡¨è¾¾å¼
class Selectors:
    BANGUMI_LINKS = 'div.m-week-square a[href*="/Home/Bangumi/"]'
    BANGUMI_ID_PATTERN = r"/Home/Bangumi/(\d+)"
    BANGUMI_SUBJECT_PATTERN = r"/subject/(\d+)"
    DATE_PATTERN = r"\d+/\d+/\d+"
    EXCLUDED_GROUPS = {"ä¸»é¡µ", "è®¢é˜…", "åˆ—è¡¨", "æœç´¢ç«™å†…", "å–æ¶ˆ"}


class MikanSpider:
    """MikanåŠ¨ç”»ç½‘ç«™çˆ¬è™«"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(Config.HEADERS)

    def fetch_html(self, url: str) -> str:
        """è·å–ç½‘é¡µHTMLå†…å®¹"""
        for attempt in range(Config.MAX_RETRIES):
            try:
                response = self.session.get(url, timeout=Config.TIMEOUT)
                response.raise_for_status()
                return response.text
            except requests.RequestException as e:
                logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{Config.MAX_RETRIES}): {e}")
                if attempt == Config.MAX_RETRIES - 1:
                    raise
        raise requests.RequestException("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")

    def parse_bangumi_list_page(self, html: str) -> List[Tuple[str, str, Optional[int]]]:
        """è§£æç•ªç»„åˆ—è¡¨é¡µé¢"""
        soup = BeautifulSoup(html, "html.parser")
        bangumi_links = []

        for link in soup.select(Selectors.BANGUMI_LINKS):
            href = link.get("href")
            title = link.get("title", "")

            if not (href and isinstance(href, str) and "/Home/Bangumi/" in href):
                continue

            mikan_id = self._extract_mikan_id(href)
            full_url = urljoin(Config.BASE_URL, href)
            bangumi_links.append((full_url, title, mikan_id))

        return bangumi_links

    def parse_bangumi_detail_page(self, html: str) -> Dict:
        """è§£æç•ªç»„è¯¦æƒ…é¡µé¢"""
        soup = BeautifulSoup(html, "html.parser")

        return {
            "title": self._extract_title(soup),
            "broadcast_day": self._extract_broadcast_day(soup),
            "broadcast_start": self._extract_broadcast_start(soup),
            "official_website": self._extract_official_website(soup),
            "bangumi_url": self._extract_bangumi_url(soup),
            "bangumi_id": self._extract_bangumi_id(soup),
            "description": self._extract_description(soup),
            "subtitle_groups": self._extract_subtitle_groups(soup),
            "resources_by_group": self._extract_resources(soup),
        }

    def _extract_mikan_id(self, href: str) -> Optional[int]:
        """æå–Mikan ID"""
        match = re.search(Selectors.BANGUMI_ID_PATTERN, href)
        return int(match.group(1)) if match else None

    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–æ ‡é¢˜"""
        # æ–¹å¼1ï¼šé¡µé¢titleæ ‡ç­¾
        if soup.title:
            title = soup.title.get_text(strip=True)
            if title.startswith("Mikan Project - "):
                title = title.replace("Mikan Project - ", "")
            return title

        # æ–¹å¼2ï¼šæŸ¥æ‰¾é¡µé¢ä¸»æ ‡é¢˜
        for heading in soup.find_all(["h1", "h2", "h3"]):
            if heading.get_text(strip=True):
                return heading.get_text(strip=True)

        return None

    def _extract_broadcast_day(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–æ”¾é€æ—¥æœŸ"""
        for text in soup.stripped_strings:
            if text.startswith("æ”¾é€æ—¥æœŸï¼š"):
                return text.replace("æ”¾é€æ—¥æœŸï¼š", "").strip()
        return None

    def _extract_broadcast_start(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–æ”¾é€å¼€å§‹æ—¶é—´"""
        for text in soup.stripped_strings:
            if text.startswith("æ”¾é€å¼€å§‹ï¼š"):
                return text.replace("æ”¾é€å¼€å§‹ï¼š", "").strip()
        return None

    def _extract_official_website(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–å®˜æ–¹ç½‘ç«™"""
        for text in soup.stripped_strings:
            if "å®˜æ–¹ç½‘ç«™" in text:
                for link in soup.find_all("a", href=True):
                    if (
                        isinstance(link, Tag)
                        and link.get_text(strip=True)
                        and link.parent
                        and isinstance(link.parent, Tag)
                        and "å®˜æ–¹ç½‘ç«™" in link.parent.get_text()
                    ):
                        href = link.get("href")
                        return str(href) if href else None
                break
        return None

    def _extract_bangumi_url(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–Bangumié“¾æ¥"""
        for link in soup.find_all("a", href=True):
            if isinstance(link, Tag):
                href = link.get("href")
                if href and "bgm.tv/subject/" in str(href):
                    return str(href)
        return None

    def _extract_bangumi_id(self, soup: BeautifulSoup) -> Optional[int]:
        """æå–Bangumi ID"""
        bangumi_url = self._extract_bangumi_url(soup)
        if bangumi_url:
            match = re.search(Selectors.BANGUMI_SUBJECT_PATTERN, bangumi_url)
            return int(match.group(1)) if match else None
        return None

    def _extract_description(self, soup: BeautifulSoup) -> Optional[str]:
        """æå–ç®€ä»‹"""
        for text in soup.stripped_strings:
            if len(text) > 50:  # ç®€ä»‹é€šå¸¸è¾ƒé•¿
                return text
        return None

    def _extract_subtitle_groups(self, soup: BeautifulSoup) -> List[Dict]:
        """æå–å­—å¹•ç»„åˆ—è¡¨ï¼ŒåŒ…å«IDå’Œåç§°"""
        subtitle_groups = []

        for text in soup.stripped_strings:
            if "å­—å¹•ç»„åˆ—è¡¨" in text:
                for ul in soup.find_all("ul"):
                    if isinstance(ul, Tag):
                        for li in ul.find_all("li"):
                            if isinstance(li, Tag):
                                # æŸ¥æ‰¾å­—å¹•ç»„é“¾æ¥
                                subgroup_link = li.find("a", class_="subgroup-name")
                                if subgroup_link and isinstance(subgroup_link, Tag):
                                    # æå–å­—å¹•ç»„ID
                                    class_name = subgroup_link.get_attribute_list("class")
                                    subgroup_id = None
                                    if class_name:
                                        for cls in class_name:
                                            if isinstance(cls, str) and cls.startswith(
                                                "subgroup-"
                                            ):
                                                try:
                                                    subgroup_id = int(cls.replace("subgroup-", ""))
                                                    break
                                                except ValueError:
                                                    continue

                                    # æå–å­—å¹•ç»„åç§°
                                    group_name = subgroup_link.get_text(strip=True)

                                    if subgroup_id and self._is_valid_subtitle_group(group_name):
                                        group_name = self._clean_subtitle_group_name(group_name)
                                        subtitle_groups.append({
                                            "id": subgroup_id,
                                            "name": group_name,
                                        })
                break

        return subtitle_groups

    def _is_valid_subtitle_group(self, group_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å­—å¹•ç»„åç§°"""
        return bool(
            group_name and len(group_name) > 1 and group_name not in Selectors.EXCLUDED_GROUPS
        )

    def _clean_subtitle_group_name(self, group_name: str) -> str:
        """æ¸…ç†å­—å¹•ç»„åç§°ï¼Œå»é™¤æ—¥æœŸä¿¡æ¯"""
        if re.search(Selectors.DATE_PATTERN, group_name):
            return re.sub(r"\s*\d+/\d+/\d+.*", "", group_name)
        return group_name

    def _extract_resources(self, soup: BeautifulSoup) -> Dict[str, List[Dict]]:
        """æå–èµ„æºåˆ—è¡¨ï¼ŒæŒ‰å­—å¹•ç»„åˆ†ç»„"""
        resources_by_group = {}

        # å…ˆè·å–æ‰€æœ‰å­—å¹•ç»„ä¿¡æ¯
        subtitle_groups = self._extract_subtitle_groups(soup)

        # ä¸ºæ¯ä¸ªå­—å¹•ç»„æå–èµ„æº
        for group_info in subtitle_groups:
            group_name = group_info["name"]
            group_id = group_info["id"]
            group_resources = self._extract_group_resources(soup, group_name, group_id)
            if group_resources:
                resources_by_group[group_name] = group_resources

        return resources_by_group

    def _extract_group_resources(
        self, soup: BeautifulSoup, group_name: str, group_id: int
    ) -> List[Dict]:
        """æå–æŒ‡å®šå­—å¹•ç»„çš„èµ„æºåˆ—è¡¨"""
        resources = []

        # æŸ¥æ‰¾åŒ…å«è¯¥å­—å¹•ç»„åç§°çš„è¡¨æ ¼
        for table in soup.find_all("table"):
            if not isinstance(table, Tag):
                continue

            # æ£€æŸ¥è¡¨æ ¼æ˜¯å¦å±äºè¯¥å­—å¹•ç»„
            table_parent = table.parent
            if table_parent and isinstance(table_parent, Tag):
                parent_text = table_parent.get_text()
                if group_name in parent_text:
                    # è§£æè¯¥è¡¨æ ¼çš„èµ„æº
                    for row in table.find_all("tr")[1:]:  # è·³è¿‡è¡¨å¤´
                        if not isinstance(row, Tag):
                            continue

                        cols = row.find_all("td")
                        if len(cols) >= 5:  # åº”è¯¥æœ‰5åˆ—ï¼šæ ‡é¢˜ã€å¤§å°ã€æ—¶é—´ã€ä¸‹è½½ã€æ’­æ”¾
                            resource = {
                                "title": cols[0].get_text(strip=True) or "",
                                "file_size": cols[1].get_text(strip=True) or "",
                                "release_date": cols[2].get_text(strip=True) or "",
                                "magnet_url": self._extract_magnet_url(cols[0]),
                                "torrent_url": self._extract_torrent_url(cols[3]),
                                "play_url": self._extract_play_url(cols[4]),
                                "subtitle_group": group_name,
                                "subtitle_group_id": group_id,
                            }
                            resources.append(resource)

        return resources

    def _extract_magnet_url(self, col: Any) -> Optional[str]:
        """æå–ç£åŠ›é“¾æ¥"""
        if not isinstance(col, Tag):
            return None
        magnet_link = col.find("a", class_="js-magnet")
        if magnet_link and isinstance(magnet_link, Tag):
            magnet_url = magnet_link.get("data-clipboard-text")
            return str(magnet_url) if magnet_url else None
        return None

    def _extract_torrent_url(self, col: Any) -> Optional[str]:
        """æå–ç§å­æ–‡ä»¶é“¾æ¥"""
        if not isinstance(col, Tag):
            return None
        torrent_link = col.find("a")
        if torrent_link and isinstance(torrent_link, Tag):
            torrent_url = torrent_link.get("href")
            if torrent_url and "Download" in str(torrent_url):
                return urljoin(Config.BASE_URL, str(torrent_url))
        return None

    def _extract_play_url(self, col: Any) -> Optional[str]:
        """æå–æ’­æ”¾é“¾æ¥"""
        if not isinstance(col, Tag):
            return None
        play_link = col.find("a")
        if play_link and isinstance(play_link, Tag):
            play_url = play_link.get("href")
            return str(play_url) if play_url else None
        return None

    def _extract_magnet_hash(self, magnet_url: Optional[str]) -> Optional[str]:
        """ä»ç£åŠ›é“¾æ¥ä¸­æå–hashå€¼"""
        if not magnet_url:
            return None
        match = re.search(r"btih:([a-fA-F0-9]{40})", magnet_url)
        return match.group(1) if match else None

    def crawl_bangumi_list(self, limit: Optional[int] = None) -> List[Dict]:
        """çˆ¬å–ç•ªç»„åˆ—è¡¨"""
        try:
            list_url = f"{Config.BASE_URL}/Home"
            html = self.fetch_html(list_url)
            bangumi_links = self.parse_bangumi_list_page(html)

            logger.info(f"å‘ç° {len(bangumi_links)} ä¸ªç•ªç»„")

            results = []
            for url, title, mikan_id in bangumi_links[:limit]:
                try:
                    logger.info(f"çˆ¬å–è¯¦æƒ…é¡µ: {title} (ID: {mikan_id})")
                    detail_html = self.fetch_html(url)
                    detail = self.parse_bangumi_detail_page(detail_html)
                    detail["mikan_id"] = mikan_id
                    results.append(detail)
                except Exception as e:
                    logger.error(f"çˆ¬å–è¯¦æƒ…é¡µå¤±è´¥ {url}: {e}")

            return results

        except Exception as e:
            logger.error(f"çˆ¬å–ç•ªç»„åˆ—è¡¨å¤±è´¥: {e}")
            return []


def main():
    """ä¸»å‡½æ•°"""
    spider = MikanSpider()

    # åˆ›å»ºçˆ¬å–æ—¥å¿—
    crawl_log = CrawlLog(spider_name="mikan_spider")

    try:
        results = spider.crawl_bangumi_list(limit=3)

        print("=" * 60)
        print("æ•°æ®éªŒè¯ç»“æœ")
        print("=" * 60)

        for i, result in enumerate(results, 1):
            print(f"\nã€åŠ¨ç”» {i}ã€‘")
            print("-" * 40)

            # éªŒè¯Animeæ¨¡å‹æ•°æ®
            mikan_id = result.get("mikan_id")
            if mikan_id is None:
                print("âŒ é”™è¯¯: mikan_id ä¸º Noneï¼Œè·³è¿‡æ­¤åŠ¨ç”»")
                continue

            anime = Anime(
                mikan_id=mikan_id,
                bangumi_id=result.get("bangumi_id"),
                title=result.get("title", ""),
                broadcast_day=result.get("broadcast_day"),
                broadcast_start=result.get("broadcast_start"),
                official_website=result.get("official_website"),
                bangumi_url=result.get("bangumi_url"),
                description=result.get("description"),
            )

            print("âœ… Animeæ¨¡å‹éªŒè¯:")
            print(f"   mikan_id: {anime.mikan_id}")
            print(f"   bangumi_id: {anime.bangumi_id}")
            print(f"   title: {anime.title}")
            print(f"   broadcast_day: {anime.broadcast_day}")
            print(f"   broadcast_start: {anime.broadcast_start}")
            print(f"   official_website: {anime.official_website}")
            print(f"   bangumi_url: {anime.bangumi_url}")
            print(
                f"   description: {anime.description[:100] + '...' if anime.description and len(anime.description) > 100 else anime.description}"
            )

            # éªŒè¯SubtitleGroupæ¨¡å‹æ•°æ®
            subtitle_groups = []
            for group_info in result.get("subtitle_groups", []):
                subtitle_group = SubtitleGroup(name=group_info["name"])
                subtitle_groups.append(subtitle_group)

            print(f"\nâœ… SubtitleGroupæ¨¡å‹éªŒè¯ (å…±{len(subtitle_groups)}ä¸ª):")
            for j, group in enumerate(subtitle_groups, 1):
                group_info = result.get("subtitle_groups", [])[j - 1]
                print(f"   {j}. {group.name} (ID: {group_info['id']})")

            # éªŒè¯Resourceæ¨¡å‹æ•°æ®
            resources = []
            resources_by_group = result.get("resources_by_group", {})

            for group_name, group_resources in resources_by_group.items():
                for resource_data in group_resources:
                    # å°è¯•ä»æ ‡é¢˜ä¸­æå–é›†æ•°
                    episode_number = 1  # é»˜è®¤å€¼
                    title = resource_data.get("title", "")

                    # ç®€å•çš„é›†æ•°æå–é€»è¾‘
                    episode_match = re.search(r"(\d+)", title)
                    if episode_match:
                        episode_number = int(episode_match.group(1))

                    resource = Resource(
                        anime_id=anime.mikan_id,  # ä½¿ç”¨mikan_idä½œä¸ºanime_id
                        subtitle_group_id=resource_data.get(
                            "subtitle_group_id", 1
                        ),  # ä½¿ç”¨å®é™…çš„subtitle_group_id
                        episode_number=episode_number,
                        title=title,
                        file_size=resource_data.get("file_size"),
                        download_url=resource_data.get("magnet_url"),  # ä½¿ç”¨ç£åŠ›é“¾æ¥ä½œä¸ºä¸‹è½½é“¾æ¥
                        magnet_hash=spider._extract_magnet_hash(resource_data.get("magnet_url")),
                        release_date=resource_data.get("release_date"),
                    )
                    resources.append(resource)

            print(f"\nâœ… Resourceæ¨¡å‹éªŒè¯ (å…±{len(resources)}ä¸ª):")
            for j, resource in enumerate(resources[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"   {j}. {resource.title}")
                print(f"      é›†æ•°: {resource.episode_number}")
                print(f"      å¤§å°: {resource.file_size}")
                print(f"      å‘å¸ƒæ—¥æœŸ: {resource.release_date}")
                print(f"      å­—å¹•ç»„ID: {resource.subtitle_group_id}")
                print(
                    f"      ç£åŠ›é“¾æ¥: {resource.download_url[:50] + '...' if resource.download_url and len(resource.download_url) > 50 else resource.download_url}"
                )

            if len(resources) > 5:
                print(f"   ... è¿˜æœ‰ {len(resources) - 5} ä¸ªèµ„æº")

            # æ›´æ–°çˆ¬å–æ—¥å¿—
            crawl_log.items_count += len(resources)
            crawl_log.status = "success"

            print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   å­—å¹•ç»„æ•°é‡: {len(subtitle_groups)}")
            print(f"   èµ„æºæ€»æ•°: {len(resources)}")

        # æœ€ç»ˆæ—¥å¿—
        crawl_log.end_time = datetime.datetime.now().isoformat()
        print("\nâœ… çˆ¬å–å®Œæˆ:")
        print(f"   æ€»èµ„æºæ•°: {crawl_log.items_count}")
        print(f"   å¼€å§‹æ—¶é—´: {crawl_log.start_time}")
        print(f"   ç»“æŸæ—¶é—´: {crawl_log.end_time}")
        print(f"   çŠ¶æ€: {crawl_log.status}")

    except Exception as e:
        crawl_log.status = "error"
        crawl_log.error_message = str(e)
        crawl_log.end_time = datetime.datetime.now().isoformat()
        print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
        print(f"   é”™è¯¯ä¿¡æ¯: {crawl_log.error_message}")
        print(f"   çŠ¶æ€: {crawl_log.status}")


if __name__ == "__main__":
    main()
