import time
from datetime import datetime, timezone
import re
from urllib.parse import quote, urljoin

from scrapy import Request, Spider

from ikuyo.crawler.items import (
    AnimeItem,
    AnimeSubtitleGroupItem,
    CrawlLogItem,
    ResourceItem,
    SubtitleGroupItem,
)
from ikuyo.utils.text_parser import (
    extract_episode_number,
    extract_resolution,
    extract_subtitle_type,
    get_current_timestamp,
    normalize_subtitle_type,
    parse_datetime_to_timestamp,
)


class MikanSpider(Spider):
    name = "mikan"

    def __init__(
        self,
        config,
        limit=None,
        start_url=None,
        mode=None,
        year=None,
        season=None,
        task_id=None,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)

        # åˆå§‹åŒ–é…ç½®
        self.config = config
        self.limit = limit
        self.start_url = start_url
        self.mode = mode or "homepage"
        self.year = year
        self.season = season
        self.task_id = task_id

        # åˆå§‹åŒ–è¿›åº¦ç›¸å…³çš„å±æ€§
        self.total_items = 0
        self.processed_items = 0
        self.progress_reporter = None
        self.start_time = time.time()  # è®°å½•çˆ¬è™«å¯åŠ¨æ—¶é—´

        # è®¾ç½®åŸºç¡€URL
        self.BASE_URL = self.config.get("mikan", {}).get("base_url", "https://mikanani.me")

        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.crawler_stats = {
            "success": 0,  # æˆåŠŸå¤„ç†çš„é¡¹ç›®æ•°
            "failed": 0,  # å¤±è´¥çš„é¡¹ç›®æ•°
            "dropped": 0,  # ä¸¢å¼ƒçš„é¡¹ç›®æ•°
        }

        self.allowed_domains = getattr(config.site, "allowed_domains", ["mikanani.me"])
        self.start_urls = getattr(config.site, "start_urls", ["https://mikanani.me/Home"])

        # åˆå§‹åŒ–çˆ¬å–æ—¥å¿—ï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
        current_timestamp = get_current_timestamp()
        self.crawl_log = CrawlLogItem()
        self.crawl_log["spider_name"] = self.name
        self.crawl_log["start_time"] = current_timestamp
        self.crawl_log["status"] = "running"
        self.crawl_log["items_count"] = 0
        self.crawl_log["crawl_mode"] = self.mode
        self.crawl_log["crawl_year"] = self.year
        self.crawl_log["crawl_season"] = self.season
        self.crawl_log["created_at"] = current_timestamp

        # å¦‚æœæŒ‡å®šäº†èµ·å§‹URLï¼Œç›´æ¥ä½¿ç”¨
        if self.start_url:
            self.start_urls = [self.start_url]
            self.logger.info(f"ä½¿ç”¨æŒ‡å®šèµ·å§‹URL: {self.start_url}")

        self.logger.info(f"çˆ¬å–æ¨¡å¼: {self.mode}")
        if self.year:
            self.logger.info(f"çˆ¬å–å¹´ä»½: {self.year}")
        if self.season:
            self.logger.info(f"çˆ¬å–å­£åº¦: {self.season}")

    def parse(self, response):
        """è§£æé¦–é¡µï¼Œæ ¹æ®çˆ¬å–æ¨¡å¼é€‰æ‹©ä¸åŒçš„è§£æç­–ç•¥"""
        try:
            # å¦‚æœæ˜¯æŒ‡å®šçš„èµ·å§‹URLï¼Œç›´æ¥è§£æè¯¦æƒ…é¡µ
            if self.start_url and "/Home/Bangumi/" in self.start_url:
                mikan_id = self._extract_mikan_id(self.start_url)
                if mikan_id:
                    self.logger.info(f"ç›´æ¥è§£ææŒ‡å®šåŠ¨ç”» (ID: {mikan_id})")
                    yield Request(
                        url=self.start_url,
                        callback=self.parse_anime_detail,
                        meta={"mikan_id": mikan_id, "title": "æŒ‡å®šåŠ¨ç”»"},
                    )
                return

            if self.mode == "homepage":
                # å…ˆè®¡ç®—æ€»æ•°
                anime_links = response.css('div.m-week-square a[href*="/Home/Bangumi/"]')
                links_to_process = anime_links[: self.limit] if self.limit else anime_links
                self.total_items = len(links_to_process)
                self.logger.info(
                    f"ğŸ“Š å‘ç° {len(anime_links)} ä¸ªåŠ¨ç”»ï¼Œå°†å¤„ç† {self.total_items} ä¸ª"
                )
                self.logger.info(f"å½“å‰total_itemså€¼: {self.total_items}")  # æ·»åŠ æ—¥å¿—

                # æŠ¥å‘Šåˆå§‹è¿›åº¦
                self._report_initial_progress()

                # ç„¶åå†å¤„ç†é¡µé¢
                yield from self.parse_homepage(response)
            elif self.mode == "year":
                yield from self.parse_by_year(response, self.year)
            elif self.mode == "season":
                yield from self.parse_by_season(response, self.year, self.season)
            elif self.mode == "full":
                yield from self.parse_full_range(response)
            else:
                self.logger.error(f"æœªçŸ¥çš„çˆ¬å–æ¨¡å¼: {self.mode}")
                yield from self.parse_homepage(response)  # é™çº§åˆ°é¦–é¡µæ¨¡å¼
        except Exception as e:
            self.logger.error(f"è§£æé¡µé¢å¤±è´¥: {str(e)}")
            self.error_message = str(e)
            return None

    def parse_homepage(self, response):
        """è§£æé¦–é¡µï¼Œè·å–åŠ¨ç”»åˆ—è¡¨"""
        self.logger.info(
            f"å¼€å§‹è§£æåŠ¨ç”»åˆ—è¡¨é¡µé¢ï¼Œé™åˆ¶çˆ¬å–æ•°é‡: {self.limit if self.limit else 'ä¸é™åˆ¶'}"
        )

        # æŸ¥æ‰¾åŠ¨ç”»é“¾æ¥
        anime_links = response.css('div.m-week-square a[href*="/Home/Bangumi/"]')
        links_to_process = anime_links[: self.limit] if self.limit else anime_links
        self.logger.info(f"ğŸš€ å³å°†ç”Ÿæˆ {self.total_items} ä¸ªå¹¶å‘è¯·æ±‚è¿›å…¥è¯¦æƒ…é¡µ")
        self.logger.info(f"parse_homepageä¸­çš„total_itemså€¼: {self.total_items}")  # æ·»åŠ æ—¥å¿—

        for link in links_to_process:
            href = link.attrib.get("href")
            title = link.attrib.get("title", "")

            if href and "/Home/Bangumi/" in href:
                mikan_id = self._extract_mikan_id(href)
                if mikan_id:
                    full_url = urljoin(self.BASE_URL, href)
                    self.logger.debug(f"ğŸ“ ç”Ÿæˆè¯·æ±‚: {title} (ID: {mikan_id})")

                    yield Request(
                        url=full_url,
                        callback=self.parse_anime_detail,
                        meta={"mikan_id": mikan_id, "title": title},
                    )

    def parse_by_year(self, response, year):
        """æŒ‰å¹´ä»½çˆ¬å–"""
        self.logger.info(f"æŒ‰å¹´ä»½çˆ¬å–: {year}å¹´")
        seasons = getattr(self.config, "seasons", ["æ˜¥", "å¤", "ç§‹", "å†¬"])
        for season in seasons:
            self.logger.info(f"çˆ¬å– {year}å¹´{season}å­£")
            yield from self.parse_by_season(response, year, season)

    def parse_by_season(self, response, year, season):
        """æŒ‰å­£åº¦çˆ¬å–"""
        self.logger.info(f"æŒ‰å­£åº¦çˆ¬å–: {year}å¹´{season}å­£")

        # æ„é€ APIç«¯ç‚¹
        api_url = (
            f"{self.BASE_URL}/Home/BangumiCoverFlowByDayOfWeek?year={year}&seasonStr={season}"
        )
        self.logger.info(f"è°ƒç”¨APIç«¯ç‚¹: {api_url}")

        yield Request(
            url=api_url,
            callback=self._parse_season_response,
            meta={"year": year, "season": season},
        )

    def _parse_season_response(self, response):
        """è§£æå­£åº¦APIå“åº”"""
        self.logger.info(f"è§£æAPIå“åº”: {response.url}")

        try:
            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºHTML
            if "text/html" in response.headers.get("Content-Type", b"").decode("utf-8", "ignore"):
                self.logger.info("APIè¿”å›HTMLï¼Œæå–åŠ¨ç”»é“¾æ¥")
                anime_links = response.css('div.an-info-group > a[href*="/Home/Bangumi/"]')
                total_links = len(anime_links)
                self.logger.info(f"ä»HTMLæå–åˆ° {total_links} ä¸ªåŠ¨ç”»")

                # æ ¹æ®limité™åˆ¶å¤„ç†æ•°é‡
                links_to_process = anime_links[: self.limit] if self.limit else anime_links
                self.total_items = len(links_to_process)
                self.logger.info(f"åº”ç”¨limité™åˆ¶ï¼Œåªå¤„ç†å‰ {self.total_items} ä¸ªåŠ¨ç”»")

                # æŠ¥å‘Šåˆå§‹è¿›åº¦
                self._report_initial_progress()

                # å¤„ç†æ¯ä¸ªåŠ¨ç”»é“¾æ¥
                for link in links_to_process:
                    href = link.attrib.get("href")
                    title = link.attrib.get("title", "")

                    if href and "/Home/Bangumi/" in href:
                        mikan_id = self._extract_mikan_id(href)
                        if mikan_id:
                            full_url = urljoin(self.BASE_URL, href)
                            self.logger.debug(f"ğŸ“ ç”Ÿæˆè¯·æ±‚: {title} (ID: {mikan_id})")

                            yield Request(
                                url=full_url,
                                callback=self.parse_anime_detail,
                                meta={"mikan_id": mikan_id, "title": title},
                            )
            else:
                self.logger.error("APIè¿”å›éHTMLå“åº”")
                self.error_message = "APIè¿”å›éHTMLå“åº”"

        except Exception as e:
            self.logger.error(f"è§£æå­£åº¦APIå“åº”å¤±è´¥: {str(e)}")
            self.error_message = str(e)

    def parse_full_range(self, response):
        """å…¨é‡çˆ¬å–"""
        self.logger.info("å¼€å§‹å…¨é‡çˆ¬å–")
        import datetime

        current_year = datetime.datetime.now().year
        year_range = getattr(self.config, "year_range", {"start": 2013, "end": current_year})
        start_year = year_range["start"]
        end_year = year_range["end"]
        self.logger.info(f"çˆ¬å–å¹´ä»½èŒƒå›´: {start_year} - {end_year}")
        for year in range(start_year, end_year + 1):
            self.logger.info(f"çˆ¬å– {year} å¹´")
            yield from self.parse_by_year(response, year)

    def _call_api_or_fallback(self, year, season):
        """è°ƒç”¨APIæ¥å£æˆ–é™çº§å¤„ç†"""
        api_config = getattr(self.config, "api", {})
        if not api_config.get("enabled", True):
            return False

        # ä½¿ç”¨æ­£ç¡®çš„APIç«¯ç‚¹æ ¼å¼
        # URLç¼–ç å­£åº¦åç§°
        season_encoded = quote(season, encoding="utf-8")
        api_url = f"{self.BASE_URL}/Home/BangumiCoverFlowByDayOfWeek?year={year}&seasonStr={season_encoded}"  # noqa: E501

        self.logger.info(f"è°ƒç”¨APIç«¯ç‚¹: {api_url}")

        try:
            # ä½¿ç”¨Scrapyçš„Requestæ¥è°ƒç”¨API
            yield Request(
                url=api_url,
                callback=self._parse_api_response,
                meta={"year": year, "season": season, "endpoint": api_url},
                dont_filter=True,
                errback=self._api_error_callback,
            )
            return True

        except Exception as e:
            self.logger.warning(f"APIè°ƒç”¨å¤±è´¥: {e}")
            return False

    def _parse_api_response(self, response):
        """è§£æAPIå“åº”"""
        year = response.meta["year"]
        season = response.meta["season"]
        endpoint = response.meta["endpoint"]

        self.logger.info(f"è§£æAPIå“åº”: {endpoint}")

        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status != 200:
            self.logger.warning(f"APIå“åº”çŠ¶æ€ç : {response.status}")
            return

        # ä¼˜å…ˆå°è¯•è§£æHTMLå“åº”ï¼ˆå› ä¸ºçœŸå®APIè¿”å›HTMLï¼‰
        content = response.text
        if "/Home/Bangumi/" in content:
            self.logger.info("APIè¿”å›HTMLï¼Œæå–åŠ¨ç”»é“¾æ¥")
            yield from self._extract_bangumi_from_html(content, year, season)
            return

        # å°è¯•è§£æJSONå“åº”ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        try:
            data = response.json()
            if self._contains_bangumi_data(data):
                self.logger.info("æˆåŠŸè§£æAPIæ•°æ®ï¼ŒåŒ…å«åŠ¨ç”»ä¿¡æ¯")
                yield from self._extract_bangumi_from_api(data, year, season)
                return
        except Exception as e:
            self.logger.warning(f"å¤‡ç”¨æ–¹æ¡ˆè§£æJSONå“åº”å¤±è´¥: {e}")
            pass

        self.logger.warning(f"æ— æ³•è§£æAPIå“åº”: {endpoint}")

    def _api_error_callback(self, failure):
        """APIè°ƒç”¨é”™è¯¯å›è°ƒ"""
        self.logger.warning(f"APIè°ƒç”¨å¤±è´¥: {failure.value}")

    def _parse_dynamic_page(self, year, season):
        """åŠ¨æ€é¡µé¢è§£æï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        self.logger.info(f"ä½¿ç”¨åŠ¨æ€é¡µé¢è§£æ: {year}å¹´{season}å­£")

        # è¿™é‡Œå¯ä»¥å®ç°Seleniumæˆ–Playwrightçš„é¡µé¢äº¤äº’
        # æš‚æ—¶è¿”å›ç©ºï¼Œå¦‚æœåç»­å‡ºç°çˆ¬å–APIå¤±è´¥å†å®ç°
        self.logger.info("åŠ¨æ€é¡µé¢è§£æåŠŸèƒ½å¾…å®ç°")
        return []

    def _contains_bangumi_data(self, data):
        """æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«åŠ¨ç”»ä¿¡æ¯"""
        if isinstance(data, dict):
            bangumi_fields = ["bangumi", "anime", "title", "mikan_id", "bangumi_id"]
            for field in bangumi_fields:
                if field in data:
                    return True
        elif isinstance(data, list):
            for item in data:
                if isinstance(item, dict) and self._contains_bangumi_data(item):
                    return True
        return False

    def _extract_bangumi_from_api(self, data, year, season):
        """ä»APIæ•°æ®ä¸­æå–åŠ¨ç”»ä¿¡æ¯"""
        # æ ¹æ®APIæ•°æ®ç»“æ„æå–åŠ¨ç”»IDå’Œæ ‡é¢˜
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„APIå“åº”æ ¼å¼æ¥å®ç°
        self.logger.info(f"ä»APIæ•°æ®æå–åŠ¨ç”»ä¿¡æ¯: {year}å¹´{season}å­£")

        # ç¤ºä¾‹å®ç°ï¼ˆéœ€è¦æ ¹æ®å®é™…APIè°ƒæ•´ï¼‰
        if isinstance(data, list):
            for item in data:
                if isinstance(item, dict):
                    mikan_id = item.get("mikan_id") or item.get("id")
                    title = item.get("title") or item.get("name")
                    if mikan_id:
                        full_url = urljoin(self.BASE_URL, f"/Home/Bangumi/{mikan_id}")
                        yield Request(
                            url=full_url,
                            callback=self.parse_anime_detail,
                            meta={
                                "mikan_id": mikan_id,
                                "title": title or f"{year}å¹´{season}å­£åŠ¨ç”»",
                            },
                        )

    def _extract_bangumi_from_html(self, html_content, year, season):
        """ä»HTMLä¸­æå–åŠ¨ç”»ä¿¡æ¯"""
        # æå–åŠ¨ç”»è¯¦æƒ…é¡µé“¾æ¥
        bangumi_links = re.findall(r"/Home/Bangumi/(\d+)", html_content)

        # æå–åŠ¨ç”»æ ‡é¢˜
        title_pattern = r'<a[^>]*href="/Home/Bangumi/\d+"[^>]*title="([^"]*)"'
        titles = re.findall(title_pattern, html_content)

        self.logger.info(f"ä»HTMLæå–åˆ° {len(bangumi_links)} ä¸ªåŠ¨ç”»")

        # åº”ç”¨limité™åˆ¶
        if self.limit:
            bangumi_links = bangumi_links[: self.limit]
            self.logger.info(f"åº”ç”¨limité™åˆ¶ï¼Œåªå¤„ç†å‰ {self.limit} ä¸ªåŠ¨ç”»")

        for i, mikan_id in enumerate(bangumi_links):
            title = titles[i] if i < len(titles) else f"{year}å¹´{season}å­£åŠ¨ç”»"
            full_url = urljoin(self.BASE_URL, f"/Home/Bangumi/{mikan_id}")

            yield Request(
                url=full_url,
                callback=self.parse_anime_detail,
                meta={"mikan_id": mikan_id, "title": title},
            )

    def parse_anime_detail(self, response):
        """è§£æåŠ¨ç”»è¯¦æƒ…é¡µé¢"""
        try:
            # ç°æœ‰çš„è§£æé€»è¾‘
            mikan_id = response.meta.get("mikan_id")
            title = response.meta.get("title")
            self.logger.info(f"ğŸ¬ å¼€å§‹è§£æåŠ¨ç”»: {title} (ID: {mikan_id})")

            current_timestamp = get_current_timestamp()

            # åˆ›å»ºAnime Itemï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
            anime = AnimeItem()
            anime["mikan_id"] = mikan_id
            anime["title"] = self._extract_title(response)
            anime["bangumi_id"] = self._extract_bangumi_id(response)
            anime["broadcast_day"] = self._extract_broadcast_day(response)
            anime["broadcast_start"] = self._parse_broadcast_start_timestamp(response)
            anime["official_website"] = self._extract_official_website(response)
            anime["bangumi_url"] = self._extract_bangumi_url(response)
            anime["description"] = self._extract_description(response)
            anime["status"] = "active"
            anime["created_at"] = current_timestamp
            anime["updated_at"] = current_timestamp

            yield anime

            # æå–å­—å¹•ç»„ä¿¡æ¯
            subtitle_groups = self._extract_subtitle_groups(response)
            for group in subtitle_groups:
                yield SubtitleGroupItem({
                    "id": group["group_id"],
                    "name": group["group_name"],
                    "last_update": current_timestamp,
                    "created_at": current_timestamp,
                })

            # æå–èµ„æºä¿¡æ¯ï¼ˆä½¿ç”¨å¢å¼ºè§£æï¼‰
            resources = self._extract_resources(response, mikan_id, subtitle_groups)

            # åˆ›å»ºåŠ¨ç”»-å­—å¹•ç»„å…³è”
            anime_subtitle_groups = {}

            for resource in resources:
                # åˆ›å»ºResourceItemï¼ˆä½¿ç”¨å¢å¼ºå­—æ®µï¼‰
                yield ResourceItem({
                    "mikan_id": resource["mikan_id"],
                    "subtitle_group_id": resource["group_id"],
                    "episode_number": resource["episode_number"],  # æ–°å¢ï¼šè§£æçš„é›†æ•°
                    "title": resource["title"],
                    "file_size": resource["size"],
                    "resolution": resource["resolution"],  # æ–°å¢ï¼šè§£æçš„åˆ†è¾¨ç‡
                    "subtitle_type": resource["subtitle_type"],  # æ–°å¢ï¼šè§£æçš„å­—å¹•ç±»å‹
                    "release_date": resource["release_timestamp"],  # ä½¿ç”¨æ—¶é—´æˆ³
                    "magnet_url": resource["magnet_link"],
                    "magnet_hash": resource["magnet_hash"],
                    "torrent_url": resource["torrent_url"],
                    "play_url": resource["play_url"],
                    "created_at": resource["created_at"],
                    "updated_at": current_timestamp,
                })

                # æ”¶é›†åŠ¨ç”»-å­—å¹•ç»„å…³è”ä¿¡æ¯
                group_id = resource["group_id"]
                if group_id not in anime_subtitle_groups:
                    anime_subtitle_groups[group_id] = {
                        "first_release_date": resource["release_timestamp"],
                        "last_update_date": resource["release_timestamp"],
                        "resource_count": 0,
                    }

                # æ›´æ–°å…³è”ä¿¡æ¯
                if resource["release_timestamp"]:
                    if (
                        anime_subtitle_groups[group_id]["first_release_date"] is None
                        or resource["release_timestamp"]
                        < anime_subtitle_groups[group_id]["first_release_date"]
                    ):
                        anime_subtitle_groups[group_id]["first_release_date"] = resource[
                            "release_timestamp"
                        ]

                    if (
                        anime_subtitle_groups[group_id]["last_update_date"] is None
                        or resource["release_timestamp"]
                        > anime_subtitle_groups[group_id]["last_update_date"]
                    ):
                        anime_subtitle_groups[group_id]["last_update_date"] = resource[
                            "release_timestamp"
                        ]

                anime_subtitle_groups[group_id]["resource_count"] += 1

            # åˆ›å»ºåŠ¨ç”»-å­—å¹•ç»„å…³è”Items
            for group_id, group_info in anime_subtitle_groups.items():
                yield AnimeSubtitleGroupItem({
                    "mikan_id": mikan_id,
                    "subtitle_group_id": group_id,
                    "first_release_date": group_info["first_release_date"],
                    "last_update_date": group_info["last_update_date"],
                    "resource_count": group_info["resource_count"],
                    "is_active": 1,
                    "created_at": current_timestamp,
                    "updated_at": current_timestamp,
                })

            # æ›´æ–°çˆ¬å–æ—¥å¿—
            self.crawl_log["items_count"] += len(resources)
            self.crawl_log["mikan_id"] = mikan_id

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.crawler_stats["success"] += 1

            # æ›´æ–°å·²å¤„ç†ç•ªå‰§æ•°é‡
            self.processed_items += 1

        except Exception as e:
            self.logger.error(f"è§£æåŠ¨ç”»è¯¦æƒ…å¤±è´¥: {str(e)}")
            self.crawler_stats["failed"] += 1
            self.error_message = str(e)

    def _extract_mikan_id(self, href):
        """æå–Mikan ID"""
        match = re.search(r"/Home/Bangumi/(\d+)", href)
        return int(match.group(1)) if match else None

    def _extract_title(self, response):
        """æå–æ ‡é¢˜"""
        # ä»é¡µé¢titleæ ‡ç­¾æå–
        title = response.css("title::text").get()
        if title and title.startswith("Mikan Project - "):
            title = title.replace("Mikan Project - ", "")
        return title

    def _extract_bangumi_id(self, response):
        """æå–Bangumi ID"""
        bangumi_url = self._extract_bangumi_url(response)
        if bangumi_url:
            match = re.search(r"/subject/(\d+)", bangumi_url)
            return int(match.group(1)) if match else None
        return None

    def _extract_bangumi_url(self, response):
        """æå–Bangumié“¾æ¥"""
        bangumi_link = response.css('a[href*="bgm.tv/subject/"]::attr(href)').get()
        return bangumi_link

    def _extract_broadcast_day(self, response):
        """æå–æ”¾é€æ—¥æœŸ"""
        # æŸ¥æ‰¾åŒ…å«"æ”¾é€æ—¥æœŸï¼š"çš„æ–‡æœ¬
        for text in response.css("*::text").getall():
            if text and "æ”¾é€æ—¥æœŸï¼š" in text:
                return text.replace("æ”¾é€æ—¥æœŸï¼š", "").strip()
        return None

    def _extract_broadcast_start(self, response):
        """æå–æ”¾é€å¼€å§‹æ—¶é—´"""
        for text in response.css("*::text").getall():
            if text and "æ”¾é€å¼€å§‹ï¼š" in text:
                # æå–å†’å·åçš„æ—¶é—´éƒ¨åˆ†
                time_part = text.replace("æ”¾é€å¼€å§‹ï¼š", "").strip()
                if time_part:
                    return time_part
        return None

    def _extract_official_website(self, response):
        """æå–å®˜æ–¹ç½‘ç«™"""
        # å°è¯•å¤šç§æ–¹æ³•æå–å®˜æ–¹ç½‘ç«™
        extraction_methods = [
            self._find_official_links_by_text,
            self._find_official_links_in_sections,
            self._find_external_links,
            self._find_specific_domains,
        ]

        for method in extraction_methods:
            result = method(response)
            if result:
                return result

        return None

    def _find_official_links_by_text(self, response):
        """é€šè¿‡æ–‡æœ¬æŸ¥æ‰¾å®˜æ–¹ç½‘ç«™é“¾æ¥"""
        # æŸ¥æ‰¾åŒ…å«"å®˜æ–¹ç½‘ç«™"æ–‡æœ¬çš„é“¾æ¥
        official_links = response.xpath(
            '//a[contains(text(), "å®˜æ–¹ç½‘ç«™") or contains(following-sibling::text(), "å®˜æ–¹ç½‘ç«™")]/@href'  # noqa: E501
        ).getall()

        # æŸ¥æ‰¾åœ¨"å®˜æ–¹ç½‘ç«™"æ–‡æœ¬é™„è¿‘çš„é“¾æ¥
        for text_node in response.xpath('//text()[contains(., "å®˜æ–¹ç½‘ç«™")]'):
            parent = text_node.xpath("..")
            links = parent.xpath(".//a/@href").getall()
            if links:
                return links[0]

        return official_links[0] if official_links else None

    def _find_official_links_in_sections(self, response):
        """åœ¨åŒ…å«å®˜æ–¹ç½‘ç«™çš„æ®µè½ä¸­æŸ¥æ‰¾é“¾æ¥"""
        official_sections = response.xpath('//p[contains(text(), "å®˜æ–¹ç½‘ç«™")]//a/@href').getall()
        return official_sections[0] if official_sections else None

    def _find_external_links(self, response):
        """æŸ¥æ‰¾å¤–éƒ¨é“¾æ¥"""
        external_links = response.css('a[href^="http"]::attr(href)').getall()
        # è¿‡æ»¤æ‰å¸¸è§çš„éå®˜æ–¹ç½‘ç«™
        exclude_domains = ["bgm.tv", "mikanani.me", "bangumi.tv"]
        for link in external_links:
            if not any(domain in link for domain in exclude_domains):
                return link
        return None

    def _find_specific_domains(self, response):
        """æŸ¥æ‰¾ç‰¹å®šåŸŸåçš„å®˜æ–¹ç½‘ç«™"""
        specific_domains = [
            "aniplex.co.jp",
            "tbs.co.jp",
            "mbs.jp",
            "tokyo-mx.jp",
            "at-x.com",
            "animax.co.jp",
        ]
        for domain in specific_domains:
            link = response.css(f'a[href*="{domain}"]::attr(href)').get()
            if link:
                return link
        return None

    def _extract_description(self, response):
        """æå–æè¿°"""
        # å°è¯•å¤šç§æ–¹æ³•æå–æè¿°
        description_selectors = [
            ".header2-desc::text",  # æ¦‚å†µä»‹ç»éƒ¨åˆ†çš„æè¿°
            ".bangumi-info p::text",
            ".description::text",
            ".summary::text",
            # æœ€åæ‰å°è¯•meta descriptionï¼ˆé€šå¸¸æ˜¯ç½‘ç«™æè¿°ï¼Œä¸æ˜¯åŠ¨ç”»æè¿°ï¼‰
            'meta[name="description"]::attr(content)',
        ]

        for selector in description_selectors:
            description = response.css(selector).get()
            if description and len(description.strip()) > 10:
                # è¿‡æ»¤æ‰ç½‘ç«™é€šç”¨æè¿°
                if "èœœæŸ‘è®¡åˆ’" not in description and "Mikan Project" not in description:
                    return description.strip()

        return None

    def _extract_subtitle_groups(self, response):
        """æå–å­—å¹•ç»„ä¿¡æ¯"""
        subtitle_groups = []

        # æŸ¥æ‰¾æ‰€æœ‰å­—å¹•ç»„div
        group_elements = response.css("div.subgroup-text")

        for element in group_elements:
            group_id = element.attrib.get("id")
            # æå–å­—å¹•ç»„åç§°ï¼ˆç¬¬ä¸€ä¸ªé“¾æ¥çš„æ–‡æœ¬ï¼‰
            group_name = element.css("a::text").get()

            if group_id and group_name:
                subtitle_groups.append({
                    "group_id": group_id,
                    "group_name": group_name.strip(),
                })

        self.logger.info(f"æå–åˆ° {len(subtitle_groups)} ä¸ªå­—å¹•ç»„")
        return subtitle_groups

    def _extract_resources(self, response, mikan_id, subtitle_groups):
        """æå–èµ„æºä¿¡æ¯"""
        resources = []

        # æŸ¥æ‰¾æ‰€æœ‰å­—å¹•ç»„div
        group_elements = response.css("div.subgroup-text")

        for group_element in group_elements:
            group_id = group_element.attrib.get("id")
            group_name = group_element.css("a::text").get()

            if not group_id or not group_name:
                continue

            # æŸ¥æ‰¾è¯¥å­—å¹•ç»„ä¸‹é¢çš„èµ„æºè¡¨æ ¼
            # å­—å¹•ç»„divåé¢ç´§è·Ÿç€çš„tableå°±æ˜¯è¯¥å­—å¹•ç»„çš„èµ„æºè¡¨æ ¼
            resource_table = group_element.xpath(
                "following-sibling::table[contains(@class, 'table')][1]"
            )

            if resource_table:
                # æå–è¡¨æ ¼ä¸­çš„æ‰€æœ‰èµ„æºè¡Œ
                resource_rows = resource_table.css("tbody tr")

                for row in resource_rows:
                    cols = row.css("td")
                    if len(cols) >= 4:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—
                        resource = self._parse_resource_row(
                            cols, mikan_id, group_id, group_name.strip()
                        )
                        if resource:
                            resources.append(resource)

        self.logger.info(f"æå–åˆ° {len(resources)} ä¸ªèµ„æº")
        return resources

    def _parse_resource_row(self, cols, mikan_id, group_id, group_name):
        """è§£æèµ„æºè¡Œ - å¢å¼ºç‰ˆ"""
        try:
            current_timestamp = get_current_timestamp()

            # æå–èµ„æºä¿¡æ¯
            # ç¬¬1åˆ—ï¼šæ ‡é¢˜ï¼ˆåŒ…å«ç£åŠ›é“¾æ¥ï¼‰
            title_element = cols[0].css("a.magnet-link-wrap")
            title = title_element.css("::text").get() or cols[0].css("::text").get()

            # ç¬¬2åˆ—ï¼šå¤§å°
            size = cols[1].css("::text").get()

            # ç¬¬3åˆ—ï¼šæ›´æ–°æ—¶é—´
            date = cols[2].css("::text").get()

            # ç¬¬4åˆ—ï¼šç£åŠ›é“¾æ¥ï¼ˆåœ¨data-clipboard-textå±æ€§ä¸­ï¼‰
            magnet_link = cols[0].css("a.js-magnet::attr(data-clipboard-text)").get()

            # æå–ç£åŠ›é“¾æ¥çš„hashå€¼
            magnet_hash = None
            if magnet_link and magnet_link.startswith("magnet:?"):
                import re

                hash_match = re.search(r"xt=urn:btih:([a-fA-F0-9]{40})", magnet_link)
                if hash_match:
                    magnet_hash = hash_match.group(1).lower()

            # ç¬¬4åˆ—ï¼šç§å­ä¸‹è½½é“¾æ¥
            torrent_url = cols[3].css("a::attr(href)").get()
            torrent_url = urljoin(self.BASE_URL, torrent_url) if torrent_url else None

            # ç¬¬5åˆ—ï¼šåœ¨çº¿æ’­æ”¾é“¾æ¥
            play_url = cols[4].css("a::attr(href)").get()
            play_url = urljoin(self.BASE_URL, play_url) if play_url else None

            if title and magnet_link:
                # ä½¿ç”¨æ–‡æœ¬è§£æå™¨å¢å¼ºä¿¡æ¯æå–
                episode_number = extract_episode_number(title)
                resolution = extract_resolution(title)
                raw_subtitle_type = extract_subtitle_type(title)
                subtitle_type = (
                    normalize_subtitle_type(raw_subtitle_type) if raw_subtitle_type else None
                )

                # è½¬æ¢æ—¥æœŸä¸ºæ—¶é—´æˆ³
                release_timestamp = None
                if date:
                    release_timestamp = parse_datetime_to_timestamp(date.strip())

                return {
                    "mikan_id": mikan_id,
                    "group_id": group_id,
                    "group_name": group_name,
                    "title": title.strip(),
                    "size": size.strip() if size else None,
                    "date": date.strip() if date else None,
                    "release_timestamp": release_timestamp,
                    "episode_number": episode_number,  # æ–°å¢ï¼šè§£æçš„é›†æ•°
                    "resolution": resolution,  # æ–°å¢ï¼šè§£æçš„åˆ†è¾¨ç‡
                    "subtitle_type": subtitle_type,  # æ–°å¢ï¼šè§£æçš„å­—å¹•ç±»å‹
                    "magnet_link": magnet_link,
                    "magnet_hash": magnet_hash,
                    "torrent_url": torrent_url,
                    "play_url": play_url,
                    "created_at": current_timestamp,
                }
        except Exception as e:
            self.logger.warning(f"è§£æèµ„æºè¡Œå¤±è´¥: {e}")

        return None

    def _parse_broadcast_start_timestamp(self, response):
        """æå–æ”¾é€å¼€å§‹æ—¶é—´å¹¶è½¬æ¢ä¸ºæ—¶é—´æˆ³"""
        broadcast_start = self._extract_broadcast_start(response)
        if broadcast_start:
            timestamp = parse_datetime_to_timestamp(broadcast_start)
            return timestamp
        return None

    def _save_crawl_log_to_database(self):
        """ï¼ˆå·²åºŸå¼ƒï¼‰ç›´æ¥å°†çˆ¬å–æ—¥å¿—ä¿å­˜åˆ°æ•°æ®åº“ï¼Œç°ç”±ä¸Šå±‚ç»Ÿä¸€å¤„ç†"""
        pass

    def closed(self, reason):
        """çˆ¬è™«å…³é—­æ—¶çš„å›è°ƒ"""
        self.logger.info(f"çˆ¬è™«å…³é—­ï¼ŒåŸå› : {reason}")

        # æ›´æ–°çˆ¬å–æ—¥å¿—
        current_timestamp = get_current_timestamp()
        self.crawl_log["end_time"] = current_timestamp
        self.crawl_log["status"] = "completed" if reason == "finished" else "failed"
        self.crawl_log["items_count"] = self.crawler_stats["success"]
        self.crawl_log["error_message"] = getattr(self, "error_message", None)

        # ä¿å­˜çˆ¬å–æ—¥å¿—
        yield self.crawl_log

    def _now(self):
        return datetime.now(timezone.utc)

    def _report_initial_progress(self):
        """æŠ¥å‘Šçˆ¬è™«çš„åˆå§‹è¿›åº¦ï¼Œåœ¨ total_items ç¡®å®šåè°ƒç”¨ã€‚"""
        if self.progress_reporter:
            processing_speed = 0  # åˆå§‹æ—¶ä¸º0
            estimated_remaining = None  # åˆå§‹æ—¶æ— æ³•ä¼°è®¡

            self.progress_reporter.report_status("running")
            self.progress_reporter.report_progress({
                "total_items": self.total_items,
                "processed_items": 0,
                "percentage": 0,
                "processing_speed": processing_speed,
                "estimated_remaining": estimated_remaining,
            })

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        print(
            f"DEBUG: MikanSpider.from_crawler called with kwargs: {kwargs}"
        )  # Added for debugging
        spider = super().from_crawler(crawler, *args, **kwargs)
        spider.crawler = crawler  # Store the crawler object

        # åˆå§‹åŒ–è¿›åº¦æŠ¥å‘Šå™¨
        if spider.task_id is not None:
            from ikuyo.core.crawler.progress_reporter import ProgressReporter

            spider.progress_reporter = ProgressReporter(spider.task_id)
            spider.logger.info(
                f"MikanSpider from_crawler: progress_reporter initialized for task {spider.task_id}"
            )
        else:
            spider.logger.warning(
                "MikanSpider from_crawler: task_id is None, progress_reporter not initialized."
            )

        return spider
